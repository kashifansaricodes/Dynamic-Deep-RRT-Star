{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "# from MLP import MLP \n",
    "import pickle\n",
    "# from data_loader import load_test_dataset, data_loader\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler object from the saved file\n",
    "with open(r\"Scalers_and_weights\\scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64 , 128 , 3 , stride = 2 , padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 , 256 , 3 , stride = 2 , padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256 , 512 , 3 , stride = 2 , padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512 , 1024 , 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024 , 2048 , 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2048 , 28 , 1)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(28, 2048, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(2048, 1024, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256,  3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "\t\tnn.Linear(input_size, 1280),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(1280, 1024),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(1024, 896),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(896, 768),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(768, 512),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(512, 384),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(384, 256),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(256, 256),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(256, 128),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(128, 64),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(64, 32),nn.PReLU(),\n",
    "\t\tnn.Linear(32, output_size))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(node, point):\n",
    "        dist = np.sqrt((node[0] - point[0])**2 + (node[1] - point[1])**2)         \n",
    "        return dist\n",
    "\n",
    "\n",
    "def goalFound(point , goal):\n",
    "        if distance(goal, point) <= 5:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.to(torch.device(\"cuda\"))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# from data_loader import Autoencoder\n",
    "# from MLP import MLP\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "# Encoding the image\n",
    "autoencoder = Autoencoder()\n",
    "Q = autoencoder.encoder\n",
    "Q.load_state_dict(torch.load(r\"Scalers_and_weights\\model_982.pkl\"))\n",
    "if torch.cuda.is_available():\n",
    "    Q = Q.to(torch.device(\"cuda\"))\n",
    "    \n",
    "# Load trained model for path generation\n",
    "mlp = MLP(32, 2)\n",
    "mlp.load_state_dict(torch.load(r\"Scalers_and_weights\\model_190.pkl\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mlp.cuda()\n",
    "\n",
    "# Load test dataset\n",
    "image_path = r\"Datasets\\images\\152.jpg\"\n",
    "path = []\n",
    "\n",
    "image = cv2.imread(image_path ,  cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "encoded_w_m=np.zeros((1,28),dtype=np.float32)\n",
    "to_tensor = transforms.ToTensor()\n",
    "torch_img = to_tensor(image)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch_img = torch_img.to(torch.device(\"cuda\")) \n",
    "\n",
    "output=Q(torch_img)\n",
    "output = output.squeeze()\n",
    "output=output.data.cpu()\n",
    "output = output.numpy()\n",
    "encoded_w_m[0] = output\n",
    "\n",
    "start_x = 20\n",
    "start_y = 100\n",
    "start = np.array([[start_x, start_y]])\n",
    "start = scaler.transform(start)\n",
    "goal_x = 40\n",
    "goal_y = 30\n",
    "goal = np.array([[goal_x, goal_y]])\n",
    "goal = scaler.transform(goal)\n",
    "# start = np.array([start_x, start_y])\n",
    "# goal = np.array([goal_x, goal_y])\n",
    "path.append(torch.tensor(np.array([start[0][0], start[0][1]])))\n",
    "# path.append(start)\n",
    "\n",
    "\n",
    "start1=torch.from_numpy(start)\n",
    "goal1=torch.from_numpy(goal)\n",
    "obs = torch.from_numpy(encoded_w_m)\n",
    "\n",
    "start1 = start1.float()\n",
    "goal1 = goal1.float()\n",
    "\n",
    "start1 = start1.squeeze()\n",
    "goal1 = goal1.squeeze()\n",
    "obs = obs.squeeze()\n",
    "\n",
    "# print(start1.shape)\n",
    "# print(goal1.shape)\n",
    "# print(obs.shape)\n",
    "# data = torch.cat((obs,start1,goal1))\n",
    "# print(data.shape)\n",
    "# Implement the Network to generate the path\n",
    "while True:\n",
    "    data = torch.cat((obs,start1,goal1))\n",
    "    data = to_var(data)\n",
    "    current = mlp(data)\n",
    "    current = current.data.cpu()\n",
    "    #print(current.numpy())\n",
    "    path.append(current)\n",
    "    start1 = current\n",
    "#     print(current)\n",
    "    current = current.numpy()\n",
    "    current = np.array([[current[0], current[1]]])\n",
    "    current = scaler.inverse_transform(current)\n",
    "#     print(\"current\",current)\n",
    "    if goalFound(numpy.array([current[0][0], current[0][1]]) , numpy.array([goal_x, goal_y])):\n",
    "        path.append(goal1)\n",
    "        break\n",
    "path.append(goal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.         100.        ]\n",
      " [ 19.34988355  99.79672563]\n",
      " [ 16.95722483  98.09593491]\n",
      " [ 20.8218307   97.25864094]\n",
      " [ 26.94833295  97.60227407]\n",
      " [ 26.43146411  96.673574  ]\n",
      " [ 37.59685913  94.96665537]\n",
      " [ 43.57956371  93.4933058 ]\n",
      " [ 47.05037578  89.65841218]\n",
      " [ 47.79335117  82.48584434]\n",
      " [ 50.85609258  80.91173846]\n",
      " [ 49.52615589  75.53183103]\n",
      " [ 48.84810388  75.81761956]\n",
      " [ 49.946431    75.17229374]\n",
      " [ 50.00566201  79.04891396]\n",
      " [ 51.04377942  81.44101921]\n",
      " [ 57.3314137   79.15921962]\n",
      " [ 50.1201957   75.49099408]\n",
      " [ 48.44241723  69.51276324]\n",
      " [ 45.4878716   67.8646719 ]\n",
      " [ 47.09754775  67.42378145]\n",
      " [ 50.30920157  65.72060339]\n",
      " [ 48.37696895  65.96680209]\n",
      " [ 45.29868197  62.42100009]\n",
      " [ 43.83022135  59.7523119 ]\n",
      " [ 46.56983265  57.53732142]\n",
      " [ 45.19504807  53.35539914]\n",
      " [ 47.79998548  55.19209921]\n",
      " [ 49.32571414  55.57984995]\n",
      " [ 50.32150189  51.38799777]\n",
      " [ 50.18069861  46.60640967]\n",
      " [ 45.30167476  41.41547581]\n",
      " [ 44.27487978  43.49963314]\n",
      " [ 44.94946683  41.0546163 ]\n",
      " [ 42.1927169   33.76817607]\n",
      " [ 39.9999998   30.00000061]\n",
      " [ 39.9999998   30.00000061]]\n"
     ]
    }
   ],
   "source": [
    "path_modified = []\n",
    "for ele in (path):\n",
    "    if not isinstance(ele, np.ndarray):\n",
    "        ele = ele.numpy()\n",
    "        ele = ele.tolist()\n",
    "    else :\n",
    "        ele = ele.tolist()\n",
    "    path_modified.append(ele)\n",
    "\n",
    "# print(path_modified)\n",
    "\n",
    "path_modified = np.array(path_modified)\n",
    "path_modified = scaler.inverse_transform(path_modified)\n",
    "print(path_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
