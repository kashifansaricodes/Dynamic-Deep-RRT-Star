{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c385cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6c6c246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 245\u001b[0m\n\u001b[1;32m    242\u001b[0m X \u001b[38;5;241m=\u001b[39m example\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    243\u001b[0m X \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m--> 245\u001b[0m dataset , targets \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment_list\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m201\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# print(max_length)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 57\u001b[0m, in \u001b[0;36mdata_loader\u001b[0;34m(environment_list, N, NP)\u001b[0m\n\u001b[1;32m     55\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m Autoencoder()\n\u001b[1;32m     56\u001b[0m Q \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mencoder\n\u001b[0;32m---> 57\u001b[0m Q\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/sj/Deep-RRT-Star-Implementation/weights/cae_encoder2.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     59\u001b[0m     Q \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    593\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 594\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    851\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mUnpickler(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    852\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 853\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:845\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    843\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 845\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:834\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    831\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    833\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m--> 834\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:151\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 151\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    153\u001b[0m             storage_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:135\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    132\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    136\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64 , 128 , 3 , stride = 2 , padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 , 256 , 3 , stride = 2 , padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256 , 512 , 3 , stride = 2 , padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512 , 1024 , 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024 , 2048 , 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2048 , 28 , 1)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(28, 2048, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(2048, 1024, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256,  3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "def data_loader(environment_list , N = 101, NP = 201):\n",
    "    \n",
    "    \n",
    "    autoencoder = Autoencoder()\n",
    "    Q = autoencoder.encoder\n",
    "    Q.load_state_dict(torch.load(r\"/home/sj/Deep-RRT-Star-Implementation/weights/cae_encoder2.pkl\"))\n",
    "    if torch.cuda.is_available():\n",
    "        Q = Q.to(torch.device(\"cuda\"))\n",
    "\n",
    "    encoded_w_m=np.zeros((N,28),dtype=np.float32)\n",
    "    index = 0\n",
    "    for env in environment_list :\n",
    "        env = int(env)\n",
    "#         print('env')\n",
    "#         print(env)\n",
    "#         print(image_number)\n",
    "        image= cv2.imread(f'/home/sj/Deep-RRT-Star-Implementation/images/{env}.jpg' , cv2.IMREAD_GRAYSCALE)\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        torch_img = to_tensor(image)\n",
    "        if torch.cuda.is_available():\n",
    "            torch_img = torch_img.to(torch.device(\"cuda\"))  # Move input tensor to GPU\n",
    "\n",
    "        output=Q(torch_img)\n",
    "        output = output.squeeze()\n",
    "        output=output.data.cpu()\n",
    "        output = output.numpy()\n",
    "        encoded_w_m[index] = output\n",
    "        index = index + 1\n",
    "    \n",
    "    \n",
    "    file_path = r\"/home/sj/Deep-RRT-Star-Implementation/output.csv\"\n",
    "    dtypes = {col: 'float32' for col in pd.read_csv(file_path, nrows=1).columns}\n",
    "    example = pd.read_csv(file_path, dtype=dtypes)\n",
    "    #print(len(example))\n",
    "    index1 = 0\n",
    "    index2 = 0\n",
    "    index3 = 1\n",
    "    path_lengths=np.zeros((N,NP + 1),dtype=np.float32)\n",
    "    prev_image_number = 0\n",
    "    max_length=0\n",
    "    while index1 < len(example) :\n",
    "        row = example.iloc[index1]\n",
    "        index_start = row['index_start']\n",
    "        index_goal = row['index_goal']\n",
    "        len_path = (index_goal - index_start) + 1\n",
    "        image_number = row['image_number']\n",
    "        index1 = index_goal + 1\n",
    "        index1 = int(index1)\n",
    "#         print('image number')\n",
    "#         print(image_number)\n",
    "        if (image_number == prev_image_number):\n",
    "            path_lengths[index2][0] = image_number\n",
    "            path_lengths[index2][index3] = len_path\n",
    "            index3 = index3 + 1\n",
    "        else :\n",
    "#             print('image number')\n",
    "#             print(prev_image_number)\n",
    "#             print(index3 - 1)\n",
    "            index3 = 1\n",
    "            index2 = index2 + 1\n",
    "            path_lengths[index2][0] = image_number\n",
    "            path_lengths[index2][index3] = len_path\n",
    "            prev_image_number = image_number\n",
    "            index3 = index3 + 1    \n",
    "        if len_path > max_length :\n",
    "            max_length = len_path\n",
    "            \n",
    "    \n",
    "    max_length = int(max_length)\n",
    "    \n",
    "    \n",
    "    path_lengths = path_lengths[:, 1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    paths=np.zeros((N,NP,max_length,2), dtype=np.float32)\n",
    "    index = 0\n",
    "    index1 = 0\n",
    "    index2 = 0\n",
    "    index3 = 0\n",
    "    prev_image_number = 0\n",
    "    while(index < len(example)) :\n",
    "        row = example.iloc[index]\n",
    "        # index_start = int(row['index_start'])\n",
    "        # index_goal = int(row['index_goal'])\n",
    "        index_start = X[index][0]\n",
    "        image_number = X[index][1]\n",
    "        index = index_goal + 1\n",
    "        if (image_number == prev_image_number):\n",
    "            index3 = 0\n",
    "            for i in range(index_start , index_goal + 1):\n",
    "                row = example.iloc[i]\n",
    "                # x = row['current_x']\n",
    "                # y = row['current_y']\n",
    "                x = X[i][0]\n",
    "                y = X[i][1]\n",
    "                #print([x , y])\n",
    "                paths[index1][index2][index3] = [x , y]\n",
    "                #paths[index1][index2][index3][1] = y\n",
    "                index3 = index3 + 1\n",
    "            index2 = index2 + 1  \n",
    "        else :\n",
    "            index1 = index1 + 1\n",
    "            index2 = 0\n",
    "            index3 = 0\n",
    "            for i in range(index_start , index_goal + 1):\n",
    "                row = example.iloc[i]\n",
    "                # x = row['current_x']\n",
    "                # y = row['current_y']\n",
    "                x = X[i][0]\n",
    "                y = X[i][1]\n",
    "                #print([x , y])\n",
    "                paths[index1][index2][index3] = [x , y]\n",
    "                #paths[index1][index2][index3][1] = y\n",
    "                index3 = index3 + 1\n",
    "            prev_image_number = image_number    \n",
    "            index2 = index2 + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset=[]\n",
    "    targets=[]\n",
    "    for i in range(N):\n",
    "        for j in range(NP):\n",
    "            if path_lengths[i][j] > 0 :\n",
    "                for m in range(0, int(path_lengths[i][j]-1)):\n",
    "                    data=np.zeros(32,dtype=np.float32)\n",
    "                    for k in range(0 , 28):\n",
    "                        data[k] = encoded_w_m[i][k]\n",
    "                    data[28] = paths[i][j][m][0]\n",
    "                    data[29] = paths[i][j][m][1]\n",
    "                    data[30] = paths[i][j][int(path_lengths[i][j]) - 1][0]\n",
    "                    data[31] = paths[i][j][int(path_lengths[i][j]) - 1][1]\n",
    "                    \n",
    "                    temp = [round(paths[i][j][m+1][0] , 6) , round(paths[i][j][m+1][1] , 6)]\n",
    "                    temp = np.array(temp)\n",
    "                    targets.append(temp)\n",
    "                    dataset.append(data)\n",
    "                    \n",
    "    data = list(zip(dataset, targets))\n",
    "    random.shuffle(data)\n",
    "    dataset,targets=zip(*data)\n",
    "    \n",
    "    return np.asarray(dataset),np.asarray(targets)\n",
    "    \n",
    "    \n",
    "file_path = r\"/home/sj/Deep-RRT-Star-Implementation/output.csv\"\n",
    "# dtypes = {col: 'float32' for col in pd.read_csv(file_path, nrows=1).columns}\n",
    "# example = pd.read_csv(file_path, dtype=dtypes)\n",
    "# index1 = 0\n",
    "# index3 = 1\n",
    "# prev_image_number = 0\n",
    "# environment_list = [0]\n",
    "# print(len(example))\n",
    "# while index1 < len(example) :\n",
    "#     row = example.iloc[index1]\n",
    "#     index_start = row['index_start']\n",
    "#     index_goal = row['index_goal']\n",
    "#     image_number = row['image_number']\n",
    "#     index1 = index_goal + 1\n",
    "#     print(index_goal)\n",
    "#     index1 = int(index1)\n",
    "#     if (image_number == prev_image_number):\n",
    "#         index3 = index3 + 1\n",
    "#     else :\n",
    "#         environment_list.append(prev_image_number)\n",
    "#         index3 = 1\n",
    "#         prev_image_number = image_number\n",
    "#         index3 = index3 + 1\n",
    "\n",
    "with open(file_path, 'r') as file2:\n",
    "    prev_image_number = 0\n",
    "    environment_list = [0]\n",
    "    reader = csv.reader(file2)\n",
    "    for row in reader:\n",
    "        if row[-1] == 'image_number':\n",
    "            continue\n",
    "        if len(row)==5:\n",
    "            environment_number =  int(row[-1])\n",
    "            if environment_number != prev_image_number:\n",
    "                environment_list.append(environment_number)\n",
    "                prev_image_number = environment_number\n",
    "print(len(environment_list))\n",
    "\n",
    "dtypes = {col: 'float32' for col in pd.read_csv(file_path, nrows=1).columns}\n",
    "example = pd.read_csv(file_path, dtype=dtypes)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = example.iloc[:, :2].values\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "dataset , targets = data_loader(environment_list,101,201)\n",
    "\n",
    "\n",
    "# print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44be9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "\t\tnn.Linear(input_size, 1280),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(1280, 1024),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(1024, 896),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(896, 768),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(768, 512),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(512, 384),nn.PReLU(),nn.Dropout(),\n",
    "\t\tnn.Linear(384, 256),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(256, 256),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(256, 128),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(128, 64),nn.PReLU(), nn.Dropout(),\n",
    "\t\tnn.Linear(64, 32),nn.PReLU(),\n",
    "\t\tnn.Linear(32, output_size))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2240679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:608.8848\n",
      "Epoch:2, Loss:508.3774\n",
      "Epoch:3, Loss:530.8115\n",
      "Epoch:4, Loss:522.7702\n",
      "Epoch:5, Loss:459.7539\n",
      "Epoch:6, Loss:453.1690\n",
      "Epoch:7, Loss:502.8882\n",
      "Epoch:8, Loss:449.5192\n",
      "Epoch:9, Loss:600.1609\n",
      "Epoch:10, Loss:462.8765\n",
      "Epoch:11, Loss:518.3183\n",
      "Epoch:12, Loss:480.0344\n",
      "Epoch:13, Loss:536.0453\n",
      "Epoch:14, Loss:613.7839\n",
      "Epoch:15, Loss:505.6393\n",
      "Epoch:16, Loss:470.3039\n",
      "Epoch:17, Loss:430.5031\n",
      "Epoch:18, Loss:466.5578\n",
      "Epoch:19, Loss:436.5826\n",
      "Epoch:20, Loss:580.5089\n",
      "Epoch:21, Loss:478.7358\n",
      "Epoch:22, Loss:541.2933\n",
      "Epoch:23, Loss:428.0184\n",
      "Epoch:24, Loss:462.7204\n",
      "Epoch:25, Loss:446.7130\n",
      "Epoch:26, Loss:422.9966\n",
      "Epoch:27, Loss:446.8363\n",
      "Epoch:28, Loss:519.1493\n",
      "Epoch:29, Loss:388.6898\n",
      "Epoch:30, Loss:437.0158\n",
      "Epoch:31, Loss:404.4762\n",
      "Epoch:32, Loss:504.2092\n",
      "Epoch:33, Loss:469.0862\n",
      "Epoch:34, Loss:481.3764\n",
      "Epoch:35, Loss:518.8210\n",
      "Epoch:36, Loss:496.2478\n",
      "Epoch:37, Loss:479.6609\n",
      "Epoch:38, Loss:443.1186\n",
      "Epoch:39, Loss:418.0693\n",
      "Epoch:40, Loss:528.8066\n",
      "Epoch:41, Loss:448.8656\n",
      "Epoch:42, Loss:463.4168\n",
      "Epoch:43, Loss:452.0267\n",
      "Epoch:44, Loss:426.4529\n",
      "Epoch:45, Loss:441.1912\n",
      "Epoch:46, Loss:488.1994\n",
      "Epoch:47, Loss:428.1984\n",
      "Epoch:48, Loss:426.4310\n",
      "Epoch:49, Loss:414.9529\n",
      "Epoch:50, Loss:466.1037\n",
      "Epoch:51, Loss:436.1311\n",
      "Epoch:52, Loss:453.5180\n",
      "Epoch:53, Loss:455.9189\n",
      "Epoch:54, Loss:440.1055\n",
      "Epoch:55, Loss:471.9579\n",
      "Epoch:56, Loss:400.1735\n",
      "Epoch:57, Loss:422.1642\n",
      "Epoch:58, Loss:449.6682\n",
      "Epoch:59, Loss:396.7425\n",
      "Epoch:60, Loss:382.1160\n",
      "Epoch:61, Loss:426.2401\n",
      "Epoch:62, Loss:410.9612\n",
      "Epoch:63, Loss:433.7960\n",
      "Epoch:64, Loss:418.9019\n",
      "Epoch:65, Loss:438.9836\n",
      "Epoch:66, Loss:442.7786\n",
      "Epoch:67, Loss:413.9320\n",
      "Epoch:68, Loss:402.8714\n",
      "Epoch:69, Loss:412.1954\n",
      "Epoch:70, Loss:471.0150\n",
      "Epoch:71, Loss:423.5540\n",
      "Epoch:72, Loss:453.2291\n",
      "Epoch:73, Loss:422.5237\n",
      "Epoch:74, Loss:442.2310\n",
      "Epoch:75, Loss:425.0580\n",
      "Epoch:76, Loss:390.5817\n",
      "Epoch:77, Loss:422.9973\n",
      "Epoch:78, Loss:440.4539\n",
      "Epoch:79, Loss:466.6495\n",
      "Epoch:80, Loss:433.2292\n",
      "Epoch:81, Loss:399.3537\n",
      "Epoch:82, Loss:401.1856\n",
      "Epoch:83, Loss:405.7569\n",
      "Epoch:84, Loss:409.1245\n",
      "Epoch:85, Loss:447.9633\n",
      "Epoch:86, Loss:394.7939\n",
      "Epoch:87, Loss:408.4875\n",
      "Epoch:88, Loss:400.9739\n",
      "Epoch:89, Loss:411.1084\n",
      "Epoch:90, Loss:399.3786\n",
      "Epoch:91, Loss:385.4499\n",
      "Epoch:92, Loss:394.9866\n",
      "Epoch:93, Loss:392.5478\n",
      "Epoch:94, Loss:393.6126\n",
      "Epoch:95, Loss:384.5199\n",
      "Epoch:96, Loss:395.3491\n",
      "Epoch:97, Loss:403.3434\n",
      "Epoch:98, Loss:412.0687\n",
      "Epoch:99, Loss:367.8582\n",
      "Epoch:100, Loss:401.2756\n",
      "Epoch:101, Loss:389.8150\n",
      "Epoch:102, Loss:383.0650\n",
      "Epoch:103, Loss:413.3802\n",
      "Epoch:104, Loss:378.3094\n",
      "Epoch:105, Loss:394.8434\n",
      "Epoch:106, Loss:365.4778\n",
      "Epoch:107, Loss:391.8399\n",
      "Epoch:108, Loss:404.9062\n",
      "Epoch:109, Loss:409.5900\n",
      "Epoch:110, Loss:383.2426\n",
      "Epoch:111, Loss:381.3029\n",
      "Epoch:112, Loss:391.4428\n",
      "Epoch:113, Loss:391.1763\n",
      "Epoch:114, Loss:396.7226\n",
      "Epoch:115, Loss:382.6482\n",
      "Epoch:116, Loss:386.8800\n",
      "Epoch:117, Loss:371.1668\n",
      "Epoch:118, Loss:400.8907\n",
      "Epoch:119, Loss:382.4703\n",
      "Epoch:120, Loss:387.6923\n",
      "Epoch:121, Loss:391.5970\n",
      "Epoch:122, Loss:435.8577\n",
      "Epoch:123, Loss:377.9399\n",
      "Epoch:124, Loss:395.4677\n",
      "Epoch:125, Loss:370.6289\n",
      "Epoch:126, Loss:390.5668\n",
      "Epoch:127, Loss:386.6037\n",
      "Epoch:128, Loss:387.3581\n",
      "Epoch:129, Loss:390.3099\n",
      "Epoch:130, Loss:381.5838\n",
      "Epoch:131, Loss:380.1413\n",
      "Epoch:132, Loss:375.1472\n",
      "Epoch:133, Loss:375.0156\n",
      "Epoch:134, Loss:403.0908\n",
      "Epoch:135, Loss:365.6373\n",
      "Epoch:136, Loss:383.4132\n",
      "Epoch:137, Loss:416.2021\n",
      "Epoch:138, Loss:400.3761\n",
      "Epoch:139, Loss:371.3780\n",
      "Epoch:140, Loss:377.8656\n",
      "Epoch:141, Loss:411.8986\n",
      "Epoch:142, Loss:389.9165\n",
      "Epoch:143, Loss:387.6025\n",
      "Epoch:144, Loss:365.6277\n",
      "Epoch:145, Loss:394.6072\n",
      "Epoch:146, Loss:374.4956\n",
      "Epoch:147, Loss:373.0743\n",
      "Epoch:148, Loss:384.7850\n",
      "Epoch:149, Loss:384.8254\n",
      "Epoch:150, Loss:391.6792\n",
      "Epoch:151, Loss:388.9865\n",
      "Epoch:152, Loss:407.8833\n",
      "Epoch:153, Loss:383.1118\n",
      "Epoch:154, Loss:398.9658\n",
      "Epoch:155, Loss:380.9074\n",
      "Epoch:156, Loss:372.2740\n",
      "Epoch:157, Loss:390.0226\n",
      "Epoch:158, Loss:412.0774\n",
      "Epoch:159, Loss:372.0572\n",
      "Epoch:160, Loss:383.9158\n",
      "Epoch:161, Loss:385.4677\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(bo,bt)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#avg_loss=avg_loss+loss.data[0]\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.to(torch.device(\"cuda\"))\n",
    "    return x\n",
    "\n",
    "def get_input(i,data,targets,bs):\n",
    "    if i+bs<len(data):\n",
    "        bi=data[i:i+bs]\n",
    "        bt=targets[i:i+bs]\n",
    "    else:\n",
    "        bi=data[i:]\n",
    "        bt=targets[i:]\n",
    "    return torch.from_numpy(bi),torch.from_numpy(bt)\n",
    "\n",
    "#dataset,targets= load_dataset()\n",
    "#I have to yet write the architecture for the model\n",
    "mlp = MLP(32, 2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mlp = mlp.to(torch.device(\"cuda\"))\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adagrad(mlp.parameters())\n",
    "\n",
    "total_loss=[]\n",
    "sm=100 \n",
    "#training the model for 1000 epochs\n",
    "for epoch in range(1000):\n",
    "    #print (\"epoch\" + str(epoch))\n",
    "    avg_loss=0\n",
    "    for i in range (0,len(dataset),100):\n",
    "        mlp.zero_grad()\n",
    "        bi,bt= get_input(i,dataset,targets,100)\n",
    "        bi=to_var(bi)\n",
    "        bt=to_var(bt)\n",
    "        bo = mlp(bi)\n",
    "        loss = criterion(bo,bt)\n",
    "        #avg_loss=avg_loss+loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')  \n",
    "#     print \"--average loss:\"\n",
    "#     print avg_loss/(len(dataset)/args.batch_size)\n",
    "    total_loss.append(avg_loss/(len(dataset)/100))\n",
    "    # Save the models\n",
    "    if epoch==sm:\n",
    "        model_path='model'+str(sm)+'.pkl'\n",
    "        save_path = '/home/sj/Deep-RRT-Star-Implementation/weights/model_weights'\n",
    "        torch.save(mlp.state_dict(),os.path.join(save_path , model_path))\n",
    "        sm=sm+50 # save model after every 50 epochs from 100 epoch ownwards\n",
    "\n",
    "            \n",
    "#Following are the commands for saving the final model.            \n",
    "model_path='/home/sj/Deep-RRT-Star-Implementation/weights/model_weights/final_model.pkl'\n",
    "torch.save(mlp.state_dict() , model_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69e389b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print(index_start)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     index1 \u001b[38;5;241m=\u001b[39m index_goal \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     index1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (image_number \u001b[38;5;241m!=\u001b[39m prev_image_number):\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         path_lengths[index2][0] = image_number\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#         path_lengths[index2][index3] = len_path\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#         index3 = index3 + 1\u001b[39;00m\n\u001b[1;32m     25\u001b[0m           \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage number\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "file_path = r\"/home/sj/Deep-RRT-Star-Implementation/output.csv\"\n",
    "# dtypes = {col: 'float32' for col in pd.read_csv(file_path, nrows=1).columns}\n",
    "example = pd.read_csv(file_path, dtype=dtypes)\n",
    "#print(len(example))\n",
    "index1 = 0\n",
    "# index2 = 0\n",
    "# index3 = 1\n",
    "# path_lengths=np.zeros((N,NP + 1),dtype=np.float32)\n",
    "prev_image_number = 0\n",
    "# max_length=0\n",
    "number_of_environments = 0\n",
    "while index1 < len(example) :\n",
    "    row = example.iloc[index1]\n",
    "    index_start = row['index_start']\n",
    "    index_goal = row['index_goal']\n",
    "#     len_path = (index_goal - index_start) + 1\n",
    "    image_number = row['image_number']\n",
    "    #print(index_start)\n",
    "    index1 = index_goal + 1\n",
    "    index1 = int(index1)\n",
    "    if (image_number != prev_image_number):\n",
    "#         path_lengths[index2][0] = image_number\n",
    "#         path_lengths[index2][index3] = len_path\n",
    "#         index3 = index3 + 1\n",
    "          print('image number')\n",
    "          print(prev_image_number)\n",
    "          print(index3 - 1)\n",
    "          prev_image_number = image_number\n",
    "#           number_of_environments = number_of_environments + 1\n",
    "#         index3 = 1\n",
    "#         index2 = index2 + 1\n",
    "#         path_lengths[index2][0] = image_number\n",
    "#         path_lengths[index2][index3] = len_path\n",
    "#         prev_image_number = image_number\n",
    "#         index3 = index3 + 1    \n",
    "#     if len_path > max_length :\n",
    "#         max_length = len_path\n",
    "\n",
    "print(number_of_environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9979f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 , 35.0\n",
      "36.0 , 62.0\n",
      "63.0 , 101.0\n",
      "102.0 , 113.0\n",
      "114.0 , 134.0\n",
      "135.0 , 141.0\n",
      "143.0 , 162.0\n",
      "nan , nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index_start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_goal)\n\u001b[1;32m     19\u001b[0m     index1 \u001b[38;5;241m=\u001b[39m index_goal \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     index1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (image_number \u001b[38;5;241m==\u001b[39m prev_image_number):\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#             path_lengths[index2][0] = image_number\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#             path_lengths[index2][index3] = len_path\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         index3 \u001b[38;5;241m=\u001b[39m index3 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "file_path = r\"/home/sj/Deep-RRT-Star-Implementation/output.csv\"\n",
    "dtypes = {col: 'float32' for col in pd.read_csv(file_path, nrows=1).columns}\n",
    "example = pd.read_csv(file_path, dtype=dtypes)\n",
    "#print(len(example))\n",
    "index1 = 0\n",
    "#     index2 = 0\n",
    "index3 = 1\n",
    "#     path_lengths=np.zeros((N,NP + 1),dtype=np.float32)\n",
    "prev_image_number = 0\n",
    "#     max_length=0\n",
    "environment_list = [0]\n",
    "while index1 < len(example) :\n",
    "    row = example.iloc[index1]\n",
    "    index_start = row['index_start']\n",
    "    index_goal = row['index_goal']\n",
    "    len_path = (index_goal - index_start) + 1\n",
    "    image_number = row['image_number']\n",
    "    print(index_start, \",\", index_goal)\n",
    "    index1 = index_goal + 1\n",
    "    index1 = int(index1)\n",
    "    if (image_number == prev_image_number):\n",
    "#             path_lengths[index2][0] = image_number\n",
    "#             path_lengths[index2][index3] = len_path\n",
    "        index3 = index3 + 1\n",
    "    else :\n",
    "#         print('image number')\n",
    "#         print(prev_image_number)\n",
    "        environment_list.append(image_number)\n",
    "#         print(index3 - 1)\n",
    "        index3 = 1\n",
    "#             index2 = index2 + 1\n",
    "#             path_lengths[index2][0] = image_number\n",
    "#             path_lengths[index2][index3] = len_path\n",
    "        prev_image_number = image_number\n",
    "        index3 = index3 + 1    \n",
    "#         if len_path > max_length :\n",
    "#             max_length = len_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "893bbe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "print(len(environment_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebd82458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "directory = r\"C:\\Users\\Navdeep\\Downloads\\images\\images\"\n",
    "for env in environment_list :\n",
    "    env = np.float32(env)\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".JPG\"):\n",
    "            numeric_part = os.path.splitext(filename)[0]\n",
    "            numeric_value = np.float32(numeric_part)\n",
    "            if (numeric_value == env):\n",
    "                image_path = os.path.join(directory, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                output_directory = r'C:\\images_modified'\n",
    "                output_path = os.path.join(output_directory, filename)\n",
    "                cv2.imwrite(output_path, image)\n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28eabbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "directory = 'C:\\images_modified'\n",
    "files = os.listdir(directory)\n",
    "num_files = len(files)\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9b918c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317938\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\Navdeep\\Downloads\\output.csv\"\n",
    "dtypes = {col: 'float32' for col in pd.read_csv(file_path, nrows=1).columns}\n",
    "example = pd.read_csv(file_path, dtype=dtypes)\n",
    "\n",
    "print(len(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c228bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
